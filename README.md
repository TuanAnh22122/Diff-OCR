ğŸ”® Diff-OCR: OCR Benchmark & Text Comparison ToolCÃ´ng cá»¥ tá»± Ä‘á»™ng Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh OCR / Text GenerationSo sÃ¡nh káº¿t quáº£ Prediction (.md) vá»›i dá»¯ liá»‡u Ground Truth (.txt) má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  hiá»‡u quáº£.ğŸ“‘ Má»¥c lá»¥cTÃ­nh nÄƒng chÃ­nhYÃªu cáº§u cÃ i Ä‘áº·tCáº¥u trÃºc thÆ° má»¥cHÆ°á»›ng dáº«n sá»­ dá»¥ngBÃ¡o cÃ¡o káº¿t quáº£ğŸš€ TÃ­nh nÄƒng chÃ­nhTÃ­nh nÄƒngMÃ´ táº£ chi tiáº¿tğŸ”„ So sÃ¡nh Ä‘a Ä‘á»‹nh dáº¡ngHá»— trá»£ so sÃ¡nh chÃ©o giá»¯a file .txt (nhÃ£n gá»‘c) vÃ  file .md (káº¿t quáº£ model).ğŸ§¹ Smart PreprocessingTá»± Ä‘á»™ng loáº¡i bá» cÃº phÃ¡p Markdown (áº£nh, link, header, bold...) vÃ  chuáº©n hÃ³a vÄƒn báº£n (lowercase, strip spaces) trÆ°á»›c khi so sÃ¡nh.ğŸ“Š Metrics chuyÃªn sÃ¢uTÃ­nh toÃ¡n tá»± Ä‘á»™ng CER (Character Error Rate), WER (Word Error Rate) vÃ  Accuracy.ğŸ“ˆ BÃ¡o cÃ¡o trá»±c quanHiá»ƒn thá»‹ báº£ng tÃ³m táº¯t trÃªn Terminal vÃ  xuáº¥t file .csv chi tiáº¿t cho tá»«ng cáº·p dá»¯ liá»‡u Ä‘á»ƒ tiá»‡n debug.ğŸ“¦ YÃªu cáº§u cÃ i Ä‘áº·tÄáº£m báº£o báº¡n Ä‘Ã£ cÃ i Ä‘áº·t Python. CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n phá»¥ thuá»™c báº±ng lá»‡nh sau:pip install pandas jiwer tqdm tabulate
ğŸ“š Tech StackPandas: Xá»­ lÃ½ dá»¯ liá»‡u báº£ng vÃ  xuáº¥t CSV.Jiwer: ThÆ° viá»‡n lÃµi tÃ­nh toÃ¡n khoáº£ng cÃ¡ch Levenshtein (CER/WER).Tqdm: Hiá»ƒn thá»‹ thanh tiáº¿n trÃ¬nh (Progress bar).Tabulate: Format báº£ng káº¿t quáº£ Ä‘áº¹p máº¯t trÃªn console.ğŸ“‚ Cáº¥u trÃºc thÆ° má»¥cÄá»ƒ tool hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c, hÃ£y tá»• chá»©c thÆ° má»¥c dá»± Ã¡n theo cáº¥u trÃºc cÃ¢y dÆ°á»›i Ä‘Ã¢y:Project_Root/
â”œâ”€â”€ ğŸ“œ compare_md_txt.py              # <--- File code chÃ­nh
â”‚
â”œâ”€â”€ ğŸ—ƒï¸ datatest/                      # (ROOT_DATASET - Chá»©a nhÃ£n gá»‘c)
â”‚   â”œâ”€â”€ datatest_in/
â”‚   â”‚   â””â”€â”€ txt/                      # Chá»©a cÃ¡c file .txt chuáº©n
â”‚   â””â”€â”€ datatest_tay/
â”‚       â””â”€â”€ txt/                      # Chá»©a cÃ¡c file .txt chuáº©n
â”‚
â””â”€â”€ ğŸ¤– evaluation_olm/                # (ROOT_EVALUATION - Chá»©a káº¿t quáº£ model)
    â”œâ”€â”€ evaluation_output_dataset_in/ # Chá»©a file .md model sinh ra
    â””â”€â”€ evaluation_output_dataset_tay/ # Chá»©a file .md model sinh ra
[!IMPORTANT] LÆ°u Ã½ quan trá»ng: File káº¿t quáº£ vÃ  file gá»‘c pháº£i cÃ³ cÃ¹ng tÃªn Ä‘á»ƒ thuáº­t toÃ¡n ghÃ©p cáº·p hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c. VÃ­ dá»¥: data_01.txt sáº½ Ä‘Æ°á»£c so khá»›p vá»›i data_01.md.âš™ï¸ HÆ°á»›ng dáº«n sá»­ dá»¥ng1ï¸âƒ£ Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«nMá»Ÿ file compare_md_txt.py vÃ  chá»‰nh sá»­a biáº¿n Ä‘Æ°á»ng dáº«n á»Ÿ cuá»‘i file náº¿u cáº§n:if __name__ == "__main__":
    # TÃªn thÆ° má»¥c chá»©a dá»¯ liá»‡u gá»‘c (Ground Truth)
    ROOT_DATASET = "datatest"
    
    # TÃªn thÆ° má»¥c chá»©a káº¿t quáº£ Ä‘Ã¡nh giÃ¡ (Prediction)
    ROOT_EVALUATION = "evaluation_olm"
2ï¸âƒ£ Cháº¡y Ä‘Ã¡nh giÃ¡Cháº¡y lá»‡nh sau táº¡i thÆ° má»¥c gá»‘c cá»§a dá»± Ã¡n:python compare_md_txt.py
3ï¸âƒ£ TÃ¹y chá»‰nh nÃ¢ng caoÄá»ƒ thÃªm táº­p dá»¯ liá»‡u má»›i, sá»­a biáº¿n tasks trong hÃ m auto_benchmark:tasks = {
    "DATASET_MOI": (
        # ÄÆ°á»ng dáº«n tá»›i file gá»‘c (join path tá»« ROOT_DATASET)
        os.path.join("folder_goc", "sub_path"), 
        
        # TÃªn folder káº¿t quáº£ (náº±m trong ROOT_EVALUATION)
        "folder_ket_qua"
    ),
}
ğŸ“Š BÃ¡o cÃ¡o káº¿t quáº£Sau khi cháº¡y, tool sáº½ xuáº¥t file CSV vá»›i cÃ¡c trÆ°á»ng thÃ´ng tin sau:TÃªn cá»™tÃ nghÄ©aGhi chÃºFilenameTÃªn file Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡Char_AccÄá»™ chÃ­nh xÃ¡c kÃ½ tá»±1 - CERWord_AccÄá»™ chÃ­nh xÃ¡c tá»«1 - WERCERCharacter Error RateCÃ ng tháº¥p cÃ ng tá»‘tGT_PreviewVÄƒn báº£n gá»‘c (Ground Truth)DÃ¹ng Ä‘á»ƒ kiá»ƒm tra nhanhPred_PreviewVÄƒn báº£n dá»± Ä‘oÃ¡n (Prediction)ÄÃ£ qua xá»­ lÃ½ lÃ m sáº¡chDeveloped for Internal Benchmarking
