# Diff-OCR
OCR Benchmark & Text Comparison Tool

CÃ´ng cá»¥ tá»± Ä‘á»™ng Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh OCR hoáº·c Text Generation báº±ng cÃ¡ch so sÃ¡nh káº¿t quáº£ Ä‘áº§u ra (Ä‘á»‹nh dáº¡ng Markdown .md) vá»›i dá»¯ liá»‡u nhÃ£n gá»‘c (Ä‘á»‹nh dáº¡ng Text .txt).

ğŸš€ TÃ­nh nÄƒng chÃ­nh

So sÃ¡nh Ä‘a Ä‘á»‹nh dáº¡ng: So sÃ¡nh ná»™i dung giá»¯a file .txt (Ground Truth) vÃ  file .md (Prediction).

LÃ m sáº¡ch dá»¯ liá»‡u (Preprocessing):

Tá»± Ä‘á»™ng loáº¡i bá» cÃº phÃ¡p Markdown (áº£nh, link, header, bold, italic, code block...).

Chuáº©n hÃ³a vÄƒn báº£n (lowercase, xÃ³a khoáº£ng tráº¯ng thá»«a) Ä‘á»ƒ so sÃ¡nh ná»™i dung thá»±c táº¿.

Metrics Ä‘Ã¡nh giÃ¡: TÃ­nh toÃ¡n CER (Character Error Rate), WER (Word Error Rate), vÃ  Ä‘á»™ chÃ­nh xÃ¡c (Accuracy).

BÃ¡o cÃ¡o tá»± Ä‘á»™ng:

Hiá»ƒn thá»‹ báº£ng káº¿t quáº£ tÃ³m táº¯t trÃªn terminal.

Xuáº¥t file CSV chi tiáº¿t cho tá»«ng cáº·p file.

ğŸ“¦ YÃªu cáº§u cÃ i Ä‘áº·t

TrÆ°á»›c khi cháº¡y, hÃ£y Ä‘áº£m báº£o báº¡n Ä‘Ã£ cÃ i Ä‘áº·t Python vÃ  cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:

pip install pandas jiwer tqdm tabulate


pandas: Xá»­ lÃ½ dá»¯ liá»‡u vÃ  xuáº¥t file CSV.

jiwer: TÃ­nh toÃ¡n CER vÃ  WER.

tqdm: Hiá»ƒn thá»‹ thanh tiáº¿n trÃ¬nh.

tabulate: Váº½ báº£ng káº¿t quáº£ Ä‘áº¹p trÃªn console.

ğŸ“‚ Cáº¥u trÃºc thÆ° má»¥c (Quan trá»ng)

Máº·c Ä‘á»‹nh, code Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘á»ƒ quÃ©t theo cáº¥u trÃºc thÆ° má»¥c sau. Báº¡n cáº§n Ä‘áº£m báº£o dá»¯ liá»‡u cá»§a mÃ¬nh khá»›p vá»›i cáº¥u trÃºc nÃ y (hoáº·c xem pháº§n TÃ¹y chá»‰nh Ä‘á»ƒ sá»­a Ä‘á»•i):

Project_Root/
â”œâ”€â”€ compare_md_txt.py          <-- File code chÃ­nh
â”‚
â”œâ”€â”€ datatest/                  <-- (ROOT_DATASET - Chá»©a nhÃ£n gá»‘c)
â”‚   â”œâ”€â”€ datatest_in/
â”‚   â”‚   â””â”€â”€ txt/               <-- Chá»©a cÃ¡c file .txt chuáº©n
â”‚   â””â”€â”€ datatest_tay/
â”‚       â””â”€â”€ txt/               <-- Chá»©a cÃ¡c file .txt chuáº©n
â”‚
â””â”€â”€ evaluation_olm/            <-- (ROOT_EVALUATION - Chá»©a káº¿t quáº£ model)
    â”œâ”€â”€ evaluation_output_dataset_in/  <-- Chá»©a file .md model sinh ra
    â””â”€â”€ evaluation_output_dataset_tay/ <-- Chá»©a file .md model sinh ra


LÆ°u Ã½: TÃªn file .txt vÃ  .md pháº£i giá»‘ng nhau Ä‘á»ƒ tool cÃ³ thá»ƒ ghÃ©p cáº·p (vÃ­ dá»¥: file_01.txt sáº½ so sÃ¡nh vá»›i file_01.md).

âš™ï¸ HÆ°á»›ng dáº«n sá»­ dá»¥ng

BÆ°á»›c 1: Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n

Má»Ÿ file compare_md_txt.py, kÃ©o xuá»‘ng dÆ°á»›i cÃ¹ng (dÃ²ng ~120) vÃ  chá»‰nh sá»­a 2 biáº¿n sau Ä‘á»ƒ trá» Ä‘Ãºng vÃ o thÆ° má»¥c cá»§a báº¡n:

if __name__ == "__main__":
    # TÃªn thÆ° má»¥c chá»©a dá»¯ liá»‡u gá»‘c (Ground Truth)
    ROOT_DATASET = "datatest" 
    
    # TÃªn thÆ° má»¥c chá»©a káº¿t quáº£ Ä‘Ã¡nh giÃ¡ (Prediction)
    ROOT_EVALUATION = "evaluation_olm"


BÆ°á»›c 2: Cháº¡y Ä‘Ã¡nh giÃ¡

Má»Ÿ terminal táº¡i thÆ° má»¥c chá»©a code vÃ  cháº¡y lá»‡nh:

python compare_md_txt.py


BÆ°á»›c 3: Xem káº¿t quáº£

TrÃªn mÃ n hÃ¬nh: Tool sáº½ hiá»ƒn thá»‹ báº£ng tÃ³m táº¯t Ä‘á»™ chÃ­nh xÃ¡c trung bÃ¬nh.

File CSV: Káº¿t quáº£ chi tiáº¿t sáº½ Ä‘Æ°á»£c lÆ°u vÃ o thÆ° má»¥c ROOT_EVALUATION vá»›i tÃªn file dáº¡ng:

benchmark_dataset_in.csv

benchmark_dataset_tay.csv

Giáº£i thÃ­ch cÃ¡c cá»™t trong file CSV:

Filename: TÃªn file Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡.

Char_Acc: Äá»™ chÃ­nh xÃ¡c kÃ½ tá»± (1 - CER).

Word_Acc: Äá»™ chÃ­nh xÃ¡c tá»« (1 - WER).

CER: Character Error Rate (cÃ ng tháº¥p cÃ ng tá»‘t).

GT_Preview: Äoáº¡n Ä‘áº§u cá»§a vÄƒn báº£n gá»‘c.

Pred_Preview: Äoáº¡n Ä‘áº§u cá»§a vÄƒn báº£n dá»± Ä‘oÃ¡n (Ä‘Ã£ lÃ m sáº¡ch).

ğŸ›  TÃ¹y chá»‰nh nÃ¢ng cao

Náº¿u cáº¥u trÃºc thÆ° má»¥c con cá»§a báº¡n khÃ¡c máº·c Ä‘á»‹nh, hÃ£y tÃ¬m Ä‘áº¿n hÃ m auto_benchmark vÃ  sá»­a biáº¿n tasks:

tasks = {
    "TÃŠN_NHIá»†M_Vá»¤": (
        # ÄÆ°á»ng dáº«n con tá»›i file gá»‘c (tÃ­nh tá»« ROOT_DATASET)
        os.path.join("folder_goc", "sub_folder"), 
        
        # TÃªn folder chá»©a káº¿t quáº£ (tÃ­nh tá»« ROOT_EVALUATION)
        "folder_ket_qua"
    ),
}

