import os
import glob
import re
import pandas as pd
from jiwer import wer, cer
from tqdm import tqdm
from tabulate import tabulate

# ================= 1. H√ÄM X·ª¨ L√ù TEXT (GI·ªÆ NGUY√äN) =================
def advanced_clean_text(text):
    if not isinstance(text, str):
        text = str(text) if text is not None else ""
    text = text.lower()
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'[#*`>\-_]', ' ', text)
    text = " ".join(text.split())
    return text

def load_file(path):
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    except:
        return ""

# ================= 2. ENGINE T·ª∞ ƒê·ªòNG QU√âT =================
def auto_benchmark(root_dataset, root_evaluation):
    """
    T·ª± ƒë·ªông gh√©p c·∫∑p c√°c folder con d·ª±a tr√™n quy t·∫Øc ƒë·∫∑t t√™n.
    """
    # C·∫•u h√¨nh quy t·∫Øc gh√©p c·∫∑p folder (Mapping)
    # Key: T√™n hi·ªÉn th·ªã b√°o c√°o
    # Value: (ƒê∆∞·ªùng d·∫´n con trong dataset g·ªëc, ƒê∆∞·ªùng d·∫´n con trong k·∫øt qu·∫£)
    tasks = {
        "DATASET IN": (
            os.path.join("datatest_in", "txt"), 
            "evaluation_output_dataset_in"
        ),
        "DATASET TAY": (
            os.path.join("datatest_tay", "txt"), 
            "evaluation_output_dataset_tay"
        )
    }

    print(f"üì° ƒêANG C·∫§U H√åNH T·ª∞ ƒê·ªòNG...")
    print(f"   üìÇ Root Dataset: {root_dataset}")
    print(f"   üìÇ Root Eval   : {root_evaluation}\n")

    for task_name, (sub_gt, sub_pred) in tasks.items():
        # T·∫°o ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
        gt_dir = os.path.join(root_dataset, sub_gt)
        pred_dir = os.path.join(root_evaluation, sub_pred)

        # Ki·ªÉm tra t·ªìn t·∫°i
        if not os.path.exists(gt_dir):
            print(f"‚ö†Ô∏è  B·ªè qua {task_name}: Kh√¥ng t√¨m th·∫•y folder g·ªëc '{sub_gt}'")
            continue
        if not os.path.exists(pred_dir):
            print(f"‚ö†Ô∏è  B·ªè qua {task_name}: Kh√¥ng t√¨m th·∫•y folder k·∫øt qu·∫£ '{sub_pred}'")
            continue

        # --- CH·∫†Y LOGIC ƒê√ÅNH GI√Å (Ph·∫ßn n√†y gi·ªëng code c≈©) ---
        print(f"{'='*60}")
        print(f"üöÄ RUNNING: {task_name}")
        
        # L·∫•y file
        gt_files = {os.path.splitext(f)[0]: os.path.join(gt_dir, f) for f in os.listdir(gt_dir) if f.endswith('.txt')}
        pred_files = {os.path.splitext(f)[0]: os.path.join(pred_dir, f) for f in os.listdir(pred_dir) if f.endswith('.md')}
        
        common = sorted(list(gt_files.keys() & pred_files.keys()))
        
        if not common:
            print("‚ùå Kh√¥ng c√≥ file chung ƒë·ªÉ so s√°nh.")
            continue

        results = []
        for name in tqdm(common, desc="Evaluating"):
            gt_txt = advanced_clean_text(load_file(gt_files[name]))
            pred_txt = advanced_clean_text(load_file(pred_files[name]))
            
            if not gt_txt: continue
            
            try:
                c_err = cer(gt_txt, pred_txt)
                w_err = wer(gt_txt, pred_txt)
                results.append({
                    "Filename": name,
                    "Char_Acc": max(0, 1 - c_err),
                    "Word_Acc": max(0, 1 - w_err),
                    "CER": c_err,
                    "GT_Preview": gt_txt[:30],
                    "Pred_Preview": pred_txt[:30]
                })
            except: pass
            
        # Xu·∫•t b√°o c√°o
        if results:
            df = pd.DataFrame(results)
            print(f"\n‚úÖ K·∫æT QU·∫¢ {task_name}:")
            headers = ["Metric", "Result"]
            data = [
                ["Avg Char Accuracy", f"{df['Char_Acc'].mean()*100:.2f}%"],
                ["Avg Word Accuracy", f"{df['Word_Acc'].mean()*100:.2f}%"]
            ]
            print(tabulate(data, headers=headers, tablefmt="fancy_grid"))
            
            # L∆∞u file
            csv_name = f"benchmark_{task_name.lower().replace(' ', '_')}.csv"
            # S·ª≠a l·ªói: d√πng bi·∫øn root_evaluation ƒë∆∞·ª£c truy·ªÅn v√†o h√†m thay v√¨ bi·∫øn global
            csv_path = os.path.join(root_evaluation, csv_name)
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            print(f"   -> ƒê√£ l∆∞u: {csv_path}")

# ================= 3. NH·∫¨P INPUT T·∫†I ƒê√ÇY (R·∫§T G·ªåN) =================
if __name__ == "__main__":
    
    # B·∫°n ch·ªâ c·∫ßn s·ª≠a ƒë√∫ng 2 d√≤ng n√†y:
    ROOT_DATASET = "datatest" 
    ROOT_EVALUATION = "evaluation_chandra"

    # Ch·∫°y
    if os.path.exists(ROOT_DATASET) and os.path.exists(ROOT_EVALUATION):
        auto_benchmark(ROOT_DATASET, ROOT_EVALUATION)
    else:
        # N·∫øu th∆∞ m·ª•c ch∆∞a t·ªìn t·∫°i, in th√¥ng b√°o h∆∞·ªõng d·∫´n thay v√¨ b√°o l·ªói c·ª©ng
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c g·ªëc.")
        print(f"Vui l√≤ng t·∫°o th∆∞ m·ª•c ho·∫∑c c·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n trong code:")
        print(f"- Dataset: {ROOT_DATASET}")
        print(f"- Eval   : {ROOT_EVALUATION}")